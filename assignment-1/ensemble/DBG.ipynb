{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c06c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\vishal j. soni\\appdata\\roaming\\python\\python37\\site-packages (from tqdm) (0.4.1)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.62.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b626a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-win_amd64.whl (7.1 MB)\n",
      "     ---------------------------------------- 7.1/7.1 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-win_amd64.whl (34.1 MB)\n",
      "     ---------------------------------------- 34.1/34.1 MB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\vishal j. soni\\miniconda3\\envs\\dbms2022\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     -------------------------------------- 307.0/307.0 KB 9.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=2f87b7fbb48ed909cd539777a50daa6edba5fbf6cf3bf0b0358801b2cb8520c1\n",
      "  Stored in directory: c:\\users\\vishal j. soni\\appdata\\local\\pip\\cache\\wheels\\46\\ef\\c3\\157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1452dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.1-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vishal j. soni\\miniconda3\\envs\\dbms2022\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vishal j. soni\\miniconda3\\envs\\dbms2022\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.29.1-py3-none-any.whl (895 kB)\n",
      "     -------------------------------------- 895.5/895.5 KB 5.7 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-win_amd64.whl (51 kB)\n",
      "     ---------------------------------------- 51.6/51.6 KB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vishal j. soni\\miniconda3\\envs\\dbms2022\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vishal j. soni\\miniconda3\\envs\\dbms2022\\lib\\site-packages (from matplotlib) (1.21.5)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.0.0-cp37-cp37m-win_amd64.whl (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vishal j. soni\\miniconda3\\envs\\dbms2022\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.29.1 kiwisolver-1.3.2 matplotlib-3.5.1 pillow-9.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c037fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy\n",
    "\n",
    "    Inputs:\n",
    "    > y_hat: pd.Series of predictions\n",
    "    > y: pd.Series of ground truth\n",
    "    Output:\n",
    "    > Returns the accuracy as float\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    The following assert checks if sizes of y_hat and y are equal.\n",
    "    Students are required to add appropriate assert checks at places to\n",
    "    ensure that the function does not fail in corner cases.\n",
    "    \"\"\"\n",
    "    assert(y_hat.size == y.size)\n",
    "    # TODO: Write here\n",
    "    True_Positive = 0\n",
    "    Total = y_hat.size\n",
    "    for i in range(y.size):\n",
    "        if y_hat[i] == y[i]:\n",
    "            True_Positive += 1\n",
    "    return float(True_Positive) / Total\n",
    "    pass\n",
    "\n",
    "def precision(y_hat, y, cls):\n",
    "    \"\"\"\n",
    "    Function to calculate the precision\n",
    "\n",
    "    Inputs:\n",
    "    > y_hat: pd.Series of predictions\n",
    "    > y: pd.Series of ground truth\n",
    "    > cls: The class chosen\n",
    "    Output:\n",
    "    > Returns the precision as float\n",
    "    \"\"\"\n",
    "    assert(y_hat.size == y.size)\n",
    "    True_Positive = 0\n",
    "    Total_Predicted_Positive = 0\n",
    "    for i in range(y.size):\n",
    "        if y_hat[i] == cls and y[i] == cls:\n",
    "            True_Positive += 1\n",
    "        if y_hat[i] == cls:\n",
    "            Total_Predicted_Positive += 1\n",
    "    return float(True_Positive) / Total_Predicted_Positive\n",
    "    pass\n",
    "\n",
    "def recall(y_hat, y, cls):\n",
    "    \"\"\"\n",
    "    Function to calculate the recall\n",
    "\n",
    "    Inputs:\n",
    "    > y_hat: pd.Series of predictions\n",
    "    > y: pd.Series of ground truth\n",
    "    > cls: The class chosen\n",
    "    Output:\n",
    "    > Returns the recall as float\n",
    "    \"\"\"\n",
    "    assert(y_hat.size == y.size)\n",
    "    Actual_Positives = 0\n",
    "    Correctly_Predicted_Positive = 0\n",
    "    for i in range(y.size):\n",
    "        if y[i] == cls and y_hat[i] == cls:\n",
    "            Correctly_Predicted_Positive += 1\n",
    "        if y[i] == cls:\n",
    "            Actual_Positives += 1\n",
    "    return float(Correctly_Predicted_Positive) / Actual_Positives\n",
    "    pass\n",
    "\n",
    "def rmse(y_hat, y):\n",
    "    \"\"\"\n",
    "    Function to calculate the root-mean-squared-error(rmse)\n",
    "\n",
    "    Inputs:\n",
    "    > y_hat: pd.Series of predictions\n",
    "    > y: pd.Series of ground truth\n",
    "    Output:\n",
    "    > Returns the rmse as float\n",
    "    \"\"\"\n",
    "    assert(y_hat.size == y.size)\n",
    "    sq = 0;\n",
    "    for i in range(y.size):\n",
    "        sq += (y[i] - y_hat[i]) ** 2\n",
    "    mean_sq = sq / y.size\n",
    "    return float(math.sqrt(float(mean_sq)))\n",
    "    pass\n",
    "\n",
    "def mae(y_hat, y):\n",
    "    \"\"\"\n",
    "    Function to calculate the mean-absolute-error(mae)\n",
    "\n",
    "    Inputs:\n",
    "    > y_hat: pd.Series of predictions\n",
    "    > y: pd.Series of ground truth\n",
    "    Output:\n",
    "    > Returns the mae as float\n",
    "    \"\"\"\n",
    "    assert(y_hat.size == y.size)\n",
    "    abs_sum = 0\n",
    "    for i in range(y.size):\n",
    "        abs_sum = abs(y[i] - y_hat[i])\n",
    "    return float(abs_sum) / y.size\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf3b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c80aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3814873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAGGING.py\n",
    "\n",
    "# from tree.base import DecisionTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree as sktree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.extmath import weighted_mode\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "\n",
    "class BaggingClassifier():\n",
    "    def __init__(self, base_estimator = DecisionTreeClassifier, n_estimators=100, max_depth=100, criterion=\"entropy\"):\n",
    "        '''\n",
    "        :param base_estimator: The base estimator model instance from which the bagged ensemble is built (e.g., DecisionTree(), LinearRegression()).\n",
    "                               You can pass the object of the estimator class\n",
    "        :param n_estimators: The number of estimators/models in ensemble.\n",
    "        '''\n",
    "        self.base_estimator = base_estimator # SKtree\n",
    "        self.max_depth = max_depth \n",
    "        self.trees = []\n",
    "        self.samples_xy = []\n",
    "        self.criterion = criterion\n",
    "        self.n_estimators = n_estimators # number of iterations/ estimators for which we will run bagging.\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Function to train and construct the BaggingClassifier\n",
    "        Inputs:\n",
    "        X: pd.DataFrame with rows as samples and columns as features (shape of X is N X P) where N is the number of samples and P is the number of columns.\n",
    "        y: pd.Series with rows corresponding to output variable (shape of Y is N)\n",
    "        \"\"\"\n",
    "        # getting data samples and learning models for all those samples:\n",
    "        for iter in range(self.n_estimators):\n",
    "            curr_X = X.sample(frac=1, axis='rows', replace=True) # sampling with replacement\n",
    "            curr_y = y[curr_X.index]\n",
    "\n",
    "            curr_X = curr_X.reset_index(drop=True) # maininting new indices\n",
    "            curr_y = curr_y.reset_index(drop=True)\n",
    "\n",
    "            tree = self.base_estimator(criterion=self.criterion) # learning new model for current sample\n",
    "            tree.fit(curr_X, curr_y) # fitting in sktree\n",
    "\n",
    "            self.trees.append(tree) # storing the tree corresponding data publicly\n",
    "            self.samples_xy.append([curr_X, curr_y])\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Funtion to run the BaggingClassifier on a data point\n",
    "        Input:\n",
    "        X: pd.DataFrame with rows as samples and columns as features\n",
    "        Output:\n",
    "        y: pd.Series with rows corresponding to output variable. THe output variable in a row is the prediction for sample in corresponding row in X.\n",
    "        \"\"\"\n",
    "        Predicted = None\n",
    "        i = 0\n",
    "        for tree in self.trees:\n",
    "            if Predicted is not None:\n",
    "                Predicted[i] = tree.predict(X)\n",
    "            else:\n",
    "                Predicted = pd.Series(tree.predict(X)).to_frame()\n",
    "            i += 1\n",
    "#         print(Predicted)\n",
    "#         print(Predicted.mode(axis = 1))\n",
    "        best_prediction = Predicted.mode(axis = 1)\n",
    "#         print(best_prediction[0])\n",
    "        return best_prediction[0]\n",
    "        pass\n",
    "\n",
    "    def plot(self, X, y):\n",
    "        \"\"\"\n",
    "        Function to plot the decision surface for BaggingClassifier for each estimator(iteration).\n",
    "        Creates two figures\n",
    "        Figure 1 consists of 1 row and `n_estimators` columns and should look similar to slide #16 of lecture\n",
    "        The title of each of the estimator should be iteration number\n",
    "\n",
    "        Figure 2 should also create a decision surface by combining the individual estimators and should look similar to slide #16 of lecture\n",
    "\n",
    "        Reference for decision surface: https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "        This function should return [fig1, fig2]\n",
    "\n",
    "        \"\"\"\n",
    "        color = [\"y\", \"r\", \"b\"]\n",
    "        Z_ls = []\n",
    "        fig1, ax1 = plt.subplots(\n",
    "            1, len(self.trees), figsize=(5*len(self.trees), 4))\n",
    "\n",
    "        x_min, x_max = X[0].min(), X[0].max()\n",
    "        x_range = x_max-x_min\n",
    "        y_min, y_max = X[1].min(), X[1].max()\n",
    "        y_range = y_max-y_min\n",
    "#         print(x_range)\n",
    "#         print(y_range)\n",
    "\n",
    "        # plotting surfaces for all the sampling iterations:\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            X_tree, y_tree = self.samples_xy[i]\n",
    "\n",
    "            xx, yy = np.meshgrid(np.arange(x_min-0.5, x_max+0.5, (x_range)/50),\n",
    "                                 np.arange(y_min-0.5, y_max+0.5, (y_range)/50))\n",
    "            \n",
    "            ax1[i].set_xlabel(\"X1\")\n",
    "            ax1[i].set_ylabel(\"X2\")\n",
    "            Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            Z_ls.append(Z)\n",
    "            c_surf = ax1[i].contourf(xx, yy, Z, cmap=plt.cm.YlOrBr)\n",
    "            fig1.colorbar(c_surf, ax=ax1[i])\n",
    "\n",
    "            for y_label in y.unique():\n",
    "                idx = y_tree == y_label\n",
    "                id = list(y_tree.cat.categories).index(y_tree[idx].iloc[0])\n",
    "                ax1[i].scatter(X_tree.loc[idx, 0], X_tree.loc[idx, 1], c=color[id],\n",
    "                               cmap=plt.cm.YlOrBr, s=30,\n",
    "                               label=\"Class_Type: \"+str(y_label))\n",
    "            ax1[i].legend()\n",
    "            ax1[i].set_title(\"Decision Tree Surface: {}\".format(i + 1))\n",
    "            \n",
    "\n",
    "        # plottinh common surface\n",
    "        fig2, ax2 = plt.subplots(1, 1, figsize=(5, 4))\n",
    "        Z_ls = np.array(Z_ls)\n",
    "        Common_surf, bleh = weighted_mode(Z_ls, np.ones(Z_ls.shape))\n",
    "        c_surf = ax2.contourf(xx, yy, Z, cmap=plt.cm.YlOrBr)\n",
    "        for y_label in y.unique():\n",
    "            idx = y == y_label\n",
    "            id = list(y.cat.categories).index(y[idx].iloc[0])\n",
    "            ax2.scatter(X.loc[idx, 0], X.loc[idx, 1], c=color[id],\n",
    "                        cmap=plt.cm.YlOrBr, s=30,\n",
    "                        label=\"Class_Type: \"+str(y_label))\n",
    "        ax2.set_xlabel(\"X1\")\n",
    "        ax2.set_ylabel(\"X2\")\n",
    "        ax2.legend()\n",
    "        ax2.set_title(\"Common Decision Surface\")\n",
    "        fig2.colorbar(c_surf, ax=ax2)\n",
    "\n",
    "        fig1.savefig(\"Q6_fig1.png\")\n",
    "        fig2.savefig(\"Q6_fig2.png\")\n",
    "        return fig1, fig2\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4ec2a0a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'information_gain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VISHAL~1.SON\\AppData\\Local\\Temp/ipykernel_41168/4250393445.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mClassifier_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mClassifier_B\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier_B\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfig1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier_B\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\VISHAL~1.SON\\AppData\\Local\\Temp/ipykernel_41168/1828531276.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# learning new model for current sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fitting in sktree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# storing the tree corresponding data publicly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dbms2022\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m         )\n\u001b[0;32m    944\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\dbms2022\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 criterion = CRITERIA_CLF[self.criterion](\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'information_gain'"
     ]
    }
   ],
   "source": [
    "# Q6_BAGGING.py\n",
    "\n",
    "\"\"\"\n",
    "The current code given is for the Assignment 2.\n",
    "> Classification\n",
    "> Regression\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from metrics import *\n",
    "\n",
    "# from ensemble.bagging import BaggingClassifier\n",
    "# from tree.base import DecisionTree\n",
    "# Or use sklearn decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from linearRegression.linearRegression import LinearRegression\n",
    "\n",
    "########### BaggingClassifier ###################\n",
    "\n",
    "N = 30\n",
    "P = 2\n",
    "NUM_OP_CLASSES = 2\n",
    "n_estimators = 3\n",
    "X = pd.DataFrame(np.abs(np.random.randn(N, P)))\n",
    "y = pd.Series(np.random.randint(NUM_OP_CLASSES, size = N), dtype=\"category\")\n",
    "\n",
    "criteria = 'information_gain'\n",
    "tree = DecisionTreeClassifier(criterion=criteria)\n",
    "Classifier_B = BaggingClassifier(base_estimator=DecisionTreeClassifier, n_estimators=n_estimators )\n",
    "Classifier_B.fit(X, y)\n",
    "y_hat = Classifier_B.predict(X)\n",
    "[fig1, fig2] = Classifier_B.plot(X, y)\n",
    "print('Criteria :', criteria)\n",
    "print('Accuracy: ', accuracy(y_hat, y))\n",
    "for cls in y.unique():\n",
    "    print('Precision: ', precision(y_hat, y, cls))\n",
    "    print('Recall: ', recall(y_hat, y, cls))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
